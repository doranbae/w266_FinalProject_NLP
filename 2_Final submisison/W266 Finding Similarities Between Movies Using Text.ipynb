{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "The goal of this project is to find out whether textual information can be a sole factor to distinguish one screenplay from another. The first part of this project will cover computing similarities between different movies based on text. The second part of this experiment will cover a simple Random Forest model to predict a movie's box-office revenue based on text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of Contents\n",
    "1. Prepare Dataset\n",
    "2. Compute Similarities using LSA, tf-idf, and cosine similarity\n",
    "3. Predict movie revenue using BOW and Random Forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare Dataset\n",
    "- Scrape and join dataset\n",
    "- Standard preprocessing\n",
    "- Additional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import csv\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "from collections import Counter\n",
    "import urllib\n",
    "import importlib\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from string import digits\n",
    "import copy\n",
    "import nltk\n",
    "from gensim import corpora, models, similarities\n",
    "import gensim\n",
    "import itertools\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "from nltk.corpus import stopwords\n",
    "import copy\n",
    "import csv\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1. Scraping and saving into csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this study, movie meta data is already scraped and saved using publically available scraping python codes\n",
    "(For example, https://github.com/skozilla/BoxOfficeMojo/tree/master/boxofficemojoAPI). \n",
    "\n",
    "From a set of movie titles and revenue (\"movie_data_625.csv\"), the following scrapes corresponding movie scrips. Run them if you want to gather it yourself, but saved files are provided in the folder so you may want to just read in those csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class upload_dataset:\n",
    "    def __init__(self):\n",
    "        self.upload_revenue = []\n",
    "        self.upload_title_list = []\n",
    "\n",
    "    def upload_movieList(self):\n",
    "        #with open(\"movie_data_romance.csv\", 'r') as f:\n",
    "        with open(\"movie_data_625.csv\", 'r') as f:\n",
    "            reader = csv.reader(f, delimiter=',')\n",
    "            for row in reader:\n",
    "                self.upload_title_list.append(row[0]) \n",
    "                self.upload_revenue.append(row[1])\n",
    "            print(\"Uploaded %s total movie lists\" %len(self.upload_title_list))\n",
    "            #print(\"Uploaded %s Box Office revenue\" %len(self.upload_revenue))\n",
    "            return self.upload_title_list, self.upload_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class scrape_data:\n",
    "    def __init__(self,upload_title_list,upload_revenue):\n",
    "        self.upload_title_list = upload_title_list\n",
    "        self.upload_revenue = upload_revenue\n",
    "        self.y = []             # y labels: global box-office for each movie\n",
    "        self.x = []             # x input: screenplay text\n",
    "        self.title_list =[]     # look-up movie titles for each x value\n",
    "        self.fail_screenplay = 0\n",
    "        self.success_screenplay = 0\n",
    "        self.y = []\n",
    "        \n",
    "    def input_data_save2file(self):  # Go through the title list and scrape screenplay text\n",
    "        for m in self.upload_title_list:\n",
    "            self.x.append(scrape_data.scrape_screenplay(self,m))\n",
    "            self.y.append(self.upload_revenue[self.upload_title_list.index(m)])\n",
    "            self.title_list.append(m)\n",
    "        \n",
    "        # Filter out none values\n",
    "        for index, item in enumerate(self.x):\n",
    "            if item == 0:\n",
    "                self.y[index] = 0\n",
    "                self.title_list[index] = 0\n",
    "\n",
    "        self.x = filter(lambda a: a != 0, self.x)\n",
    "        #print(self.x)\n",
    "        #print(type(self.x))\n",
    "        self.y = filter(lambda a: a != 0, self.y)\n",
    "        #print(self.y)\n",
    "        #print(type(self.y))\n",
    "        self.title_list = filter(lambda a: a!= 0, self.title_list)\n",
    "        #print(self.title_list)\n",
    "        #print(type(self.title_list))\n",
    "\n",
    "        # Test:\n",
    "        with open('625_x.csv', 'wb') as f:\n",
    "            writer = csv.writer(f, delimiter = ',')\n",
    "            writer.writerow(self.x)\n",
    "\n",
    "        with open('625_y.csv', 'wb') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter = ',')\n",
    "            writer.writerow(self.y)\n",
    "        \n",
    "        with open('625_title.csv', 'wb') as f:\n",
    "            writer = csv.writer(f, delimiter = ',')\n",
    "            writer.writerow(self.title_list)\n",
    "        \n",
    "        # Entire set\n",
    "        print('Number of success movies: ', self.success_screenplay)\n",
    "        print('Number of fail movies: ', self.fail_screenplay)\n",
    "        #return self.x, self.y, self.title_list\n",
    "\n",
    "    def scrape_screenplay(self, movie_title):\n",
    "        try:\n",
    "            url = 'http://www.imsdb.com/scripts/%s.html' % movie_title\n",
    "            page = urllib.urlopen(url)\n",
    "            soup = BeautifulSoup(page.read())\n",
    "            rawtext = str(soup.find(\"td\", {\"class\": \"scrtext\"}))\n",
    "            clean = re.sub('<[^<]+?>','',rawtext)\n",
    "            clean = re.sub('[^a-zA-Z0-9 \\n\\.]', '', clean)\n",
    "            clean = re.sub('&nbsp','',clean)\n",
    "            words = clean.split()\n",
    "            if len(words) > 200:\n",
    "                self.success_screenplay += 1\n",
    "                results = \" \".join(words) # Back into string separated by space\n",
    "                return results\n",
    "            else: \n",
    "                self.fail_screenplay += 1\n",
    "                #print(\"%s movie screenplay is not available\" %movie_title)\n",
    "                return 0\n",
    "        except:  # If movie URL is dead, skip it\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "Uncomment the following if you want to gather dataset yourself. However, the data is already downloaded and provided as csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of success movies: ', 436)\n",
      "('Number of fail movies: ', 188)\n"
     ]
    }
   ],
   "source": [
    "#upload = upload_dataset()\n",
    "#upload_title_list, upload_revenue = upload.upload_movieList()\n",
    "#step_one = scrape_data(upload_title_list,upload_revenue)\n",
    "#step_one.input_data_save2file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, please read in the folloiwng csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187733202\n",
      "437\n"
     ]
    }
   ],
   "source": [
    "# Load y-value (movie revenue)\n",
    "\n",
    "with open('625_y.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, delimiter = ',')\n",
    "    temp_y = list(reader)\n",
    "\n",
    "y_boxoffice = []\n",
    "for sublist in temp_y:\n",
    "    for val in sublist:\n",
    "        y_boxoffice.append(val)\n",
    "print(y_boxoffice[1])\n",
    "print(len(y_boxoffice)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load x-value (movie text)\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "with open('625_x.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, delimiter = ',')\n",
    "    temp_x = list(reader)\n",
    "\n",
    "x_scrtext = []\n",
    "for sublist in temp_x:\n",
    "    for val in sublist:\n",
    "        x_scrtext.append(val)\n",
    "#print(len(x_scrtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500-Days-of-Summer\n",
      "437\n"
     ]
    }
   ],
   "source": [
    "# Load titles for reference.\n",
    "\n",
    "with open('625_title.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, delimiter = ',')\n",
    "    temp_title = list(reader)\n",
    "\n",
    "movie_title = []\n",
    "for sublist in temp_title:\n",
    "    for val in sublist:\n",
    "        movie_title.append(val)\n",
    "print(movie_title[0])\n",
    "print(len(movie_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2. Preprocessing (Standard + Additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessing_removeName(movie_text):\n",
    "    movie_text_1 = []\n",
    "    progress = 0\n",
    "    for movie in movie_text:\n",
    "        movie_split = movie.split()\n",
    "        #print('Originally this many words: ',len(movie_split))\n",
    "        for single_word in movie_split:\n",
    "            try: \n",
    "                for w in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(single_word))):\n",
    "                    if w.label() == 'PERSON':\n",
    "                        movie_split.remove(single_word)\n",
    "            except:\n",
    "                continue\n",
    "        #print('After moving names:', len(movie_split))\n",
    "        movie_text_1.append(movie_split)\n",
    "        progress += 1\n",
    "        if progress%10 == 0:\n",
    "            print('This many done: ', progress)\n",
    "    #print(len(movie_text_1))\n",
    "    #print(type(movie_text_1))\n",
    "    return movie_text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This many done: ', 10)\n",
      "('This many done: ', 20)\n",
      "('This many done: ', 30)\n",
      "('This many done: ', 40)\n",
      "('This many done: ', 50)\n",
      "('This many done: ', 60)\n",
      "('This many done: ', 70)\n",
      "('This many done: ', 80)\n",
      "('This many done: ', 90)\n",
      "('This many done: ', 100)\n",
      "('This many done: ', 110)\n",
      "('This many done: ', 120)\n",
      "('This many done: ', 130)\n",
      "('This many done: ', 140)\n",
      "('This many done: ', 150)\n",
      "('This many done: ', 160)\n",
      "('This many done: ', 170)\n",
      "('This many done: ', 180)\n",
      "('This many done: ', 190)\n",
      "('This many done: ', 200)\n",
      "('This many done: ', 210)\n",
      "('This many done: ', 220)\n",
      "('This many done: ', 230)\n",
      "('This many done: ', 240)\n",
      "('This many done: ', 250)\n",
      "('This many done: ', 260)\n",
      "('This many done: ', 270)\n",
      "('This many done: ', 280)\n",
      "('This many done: ', 290)\n",
      "('This many done: ', 300)\n",
      "('This many done: ', 310)\n",
      "('This many done: ', 320)\n",
      "('This many done: ', 330)\n",
      "('This many done: ', 340)\n",
      "('This many done: ', 350)\n",
      "('This many done: ', 360)\n",
      "('This many done: ', 370)\n",
      "('This many done: ', 380)\n",
      "('This many done: ', 390)\n",
      "('This many done: ', 400)\n",
      "('This many done: ', 410)\n",
      "('This many done: ', 420)\n",
      "('This many done: ', 430)\n"
     ]
    }
   ],
   "source": [
    "x_scrtext = preprocessing_removeName(x_scrtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The above process took a while, so better save it and call it later. \n",
    "with open('625_x_removeName.csv', 'wb') as f:\n",
    "            writer = csv.writer(f, delimiter = ',')\n",
    "            writer.writerow(x_scrtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(x_scrtext[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437\n"
     ]
    }
   ],
   "source": [
    "# THEN OPEN IT BACK ... \n",
    "with open('625_x_removeName.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, delimiter = ',')\n",
    "    temp_x = list(reader)\n",
    "\n",
    "x_scrtext_sample = []\n",
    "for sublist in temp_x:\n",
    "    for val in sublist:\n",
    "        x_scrtext_sample.append(sublist)\n",
    "print(len(x_scrtext_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make it flat. for some reason...\n",
    "flat_x_scrtext = x_scrtext_sample[0]\n",
    "#print(flat_x_scrtext[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MovieTransform1 = []\n",
    "for movie in flat_x_scrtext:\n",
    "    new = movie.replace(',', '')\n",
    "    new1 = new.replace('.', '')\n",
    "    new2 = new1.replace(\"'\", \"\")\n",
    "    new3 = new2.replace(\"[\", \"\")\n",
    "    new4 = new3.replace(\"]\", \"\")\n",
    "    new5 = new4.split()\n",
    "    MovieTransform1.append(new5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMER\n"
     ]
    }
   ],
   "source": [
    "print(MovieTransform1[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_removeScene(movie_text):\n",
    "    movie_text2 = []\n",
    "    progress = 0\n",
    "    for movie in movie_text:\n",
    "        #print(type(movie))\n",
    "        #print('Originally this many words:', len(movie))\n",
    "        for w in movie:\n",
    "            #print(w)\n",
    "            if w.isupper():\n",
    "                movie.remove(w)\n",
    "        #print('After removing scene direcitions:', len(movie))\n",
    "        movie_text2.append(movie)\n",
    "        progress += 1\n",
    "        if progress%50 == 0:\n",
    "            print('This many done: ', progress)\n",
    "    #print(len(movie_text2))\n",
    "    #print(type(movie_text2))\n",
    "    return(movie_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This many done: ', 50)\n",
      "('This many done: ', 100)\n",
      "('This many done: ', 150)\n",
      "('This many done: ', 200)\n",
      "('This many done: ', 250)\n",
      "('This many done: ', 300)\n",
      "('This many done: ', 350)\n",
      "('This many done: ', 400)\n"
     ]
    }
   ],
   "source": [
    "x_scrtext_2 = preprocessing_removeScene(MovieTransform1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(x_scrtext_2[436])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This many done: ', 50)\n",
      "('This many done: ', 100)\n",
      "('This many done: ', 150)\n",
      "('This many done: ', 200)\n",
      "('This many done: ', 250)\n",
      "('This many done: ', 300)\n",
      "('This many done: ', 350)\n",
      "('This many done: ', 400)\n"
     ]
    }
   ],
   "source": [
    "# do it one more time\n",
    "x_scrtext_3 = preprocessing_removeScene(x_scrtext_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18019\n",
      "18019\n"
     ]
    }
   ],
   "source": [
    "print(len(x_scrtext_2[436]))\n",
    "print(len(x_scrtext_3[436]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessing_stemmer(movie_text):\n",
    "    movie_text4 = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for movie in movie_text:\n",
    "        tmp_movie = []\n",
    "        for w in movie:\n",
    "            w = w.lower()\n",
    "            if w not in stop_words:\n",
    "                tmp_movie.append(stemmer.stem(w))\n",
    "        movie_text4.append(tmp_movie)\n",
    "    print(len(movie_text4))\n",
    "    print(type(movie_text4))\n",
    "    return movie_text4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "x_scrtext_4 = preprocessing_stemmer(x_scrtext_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18019\n",
      "10035\n"
     ]
    }
   ],
   "source": [
    "print(len(x_scrtext_3[436]))\n",
    "print(len(x_scrtext_4[436]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(x_scrtext_4[436])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute Similarities\n",
    "- Create the dictionary and corpus\n",
    "- Apply LSA methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the dictionary\n",
    "dictionary = corpora.Dictionary(x_scrtext_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'fawn', 5331),\n",
       " (u'circuitri', 20598),\n",
       " (u'mmmpossibl', 79999),\n",
       " (u'schlegel', 9287),\n",
       " (u'sonji', 12699),\n",
       " (u'auggi', 83467),\n",
       " (u'mustachio', 43063),\n",
       " (u'woodi', 17913),\n",
       " (u'grandkid', 54710),\n",
       " (u'alrahman', 85306)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.islice(dictionary.token2id.items(), 0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in x_scrtext_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 8), (2, 1), (3, 11), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 11)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'enjoy'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply tf-idf transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.00023454463167326504), (1, 0.0018155088528265197), (2, 0.014476316160116187), (3, 0.007628791419793525), (4, 0.008366429014692997), (5, 0.0016735061875926688), (6, 0.011650254164639015), (7, 0.013497337656869264), (8, 0.010200649417867285), (9, 0.0007060137287942953)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus_tfidf[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating LSA Model:\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word = dictionary, num_topics = 30)\n",
    "index = similarities.MatrixSimilarity(lsi[corpus_tfidf])\n",
    "\n",
    "# Make it into dataframe\n",
    "movie = pd.DataFrame(\n",
    "    {'title': movie_title,\n",
    "     'boxoffice': y_boxoffice,\n",
    "     'text': x_scrtext_4\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add similarity score\n",
    "\n",
    "movie['similarity'] = 'unknown'\n",
    "movie['size_similar'] = 0\n",
    "total_sims = [] # storage of all similarity vectors to analysis\n",
    "threshold = 0.2\n",
    "for i, doc in enumerate(corpus_tfidf):\n",
    "    vec_lsi = lsi[doc] # convert the vector to LSI space\n",
    "    sims = index[vec_lsi] # perform a similarity vector against the corpus\n",
    "    total_sims = np.concatenate([total_sims, sims])\n",
    "    similarity = [] # Create a list with movie_id and similarity value\n",
    "    for j, x in enumerate(movie.title):\n",
    "        if sims[j] > threshold:\n",
    "            similarity.append((x, sims[j]))\n",
    "    similarity = sorted(similarity, key=lambda item: -item[1])\n",
    "    movie = movie.set_value(i, 'similarity', similarity)\n",
    "    movie = movie.set_value(i, 'size_similar', len(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_similarity = movie[['title', 'similarity']]\n",
    "db_similarity.to_csv('625_similarity.csv', sep = '|') #Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Predict Movie Revenue\n",
    "- Transform movie revenue into 3 scale (3 being hit, 1 being flop)\n",
    "- Create BOW\n",
    "- Train RF and predict movie revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hit_or_flop(boxoffice):\n",
    "    boxoffice_range = copy.copy(boxoffice)\n",
    "    for idx,b in enumerate(boxoffice_range):\n",
    "        if boxoffice_range[idx] > '300000000':\n",
    "            boxoffice_range[idx] = 3\n",
    "        elif boxoffice_range[idx] > '100000000' and boxoffice_range[idx] < '300000000':\n",
    "            boxoffice_range[idx] = 2\n",
    "        elif boxoffice_range[idx] < '100000000':\n",
    "            boxoffice_range[idx] = 1\n",
    "    return boxoffice_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxoffice_range = hit_or_flop(y_boxoffice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(boxoffice_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class train_and_test:\n",
    "    def __init__(self,scrtext, boxoffice_range, movieList):\n",
    "        self.x = scrtext\n",
    "        self.y = boxoffice_range\n",
    "        self.movieList = movieList\n",
    "        \n",
    "    def split_train_test(self,train_portion):\n",
    "        self.train_set_x = self.x[:train_portion]\n",
    "        self.test_set_x = self.x[train_portion:]\n",
    "        self.train_set_y = self.y[:train_portion]\n",
    "        self.test_set_y = self.y[train_portion:]\n",
    "        self.train_movieList = self.movieList[:train_portion]\n",
    "        self.test_movieList = self.movieList[train_portion:]\n",
    "        print(\"Train set %f\" %len(self.train_set_x))\n",
    "        print(\"Test set %f\" % len(self.test_set_x))\n",
    "\n",
    "    def feature_extraction(self):\n",
    "        vectorizer = CountVectorizer(analyzer= 'word',\n",
    "                                     tokenizer= None,\n",
    "                                     preprocessor= None,\n",
    "                                     stop_words= 'english',\n",
    "                                     max_features= 700)\n",
    "\n",
    "        self.train_data_features = vectorizer.fit_transform(self.train_set_x)\n",
    "        self.train_data_features = self.train_data_features.toarray()\n",
    "        \n",
    "        self.test_data_features = vectorizer.fit_transform(self.test_set_x)\n",
    "        self.test_data_features = self.test_data_features.toarray()\n",
    "        print(\"Training data array looks like this:\", self.train_data_features.shape)\n",
    "        print(\"Test data array looks like this:\", self.test_data_features.shape)\n",
    "\n",
    "    def train_randomForest(self):\n",
    "        # Initialize RF with 100 trees\n",
    "        forest = RandomForestClassifier(n_estimators=300)\n",
    "\n",
    "        # Fit the forest to the training set, using the bag of words as features\n",
    "        # and box office labels as the response variable\n",
    "        self.forest = forest.fit(self.train_data_features, self.train_set_y)\n",
    "        print(\"Finished training the data\")\n",
    "\n",
    "    def make_prediction(self): \n",
    "        self.result = self.forest.predict(self.test_data_features)\n",
    "        self.output = pd.DataFrame(data={\"box-office\": self.result})\n",
    "        print(self.output)\n",
    "    \n",
    "    def score(self):\n",
    "        self.pscore = metrics.accuracy_score(self.test_set_y, self.result)\n",
    "        #self.pscore_train = metrics.accuracy_score(y_train, pred_train)\n",
    "        print(self.pscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_text = []\n",
    "for movie in x_scrtext_4:\n",
    "    results = \" \".join([w for w in movie])\n",
    "    #print(len(results))\n",
    "    movie_text.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437\n"
     ]
    }
   ],
   "source": [
    "print(len(movie_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set 300.000000\n",
      "Test set 137.000000\n",
      "('Training data array looks like this:', (300, 700))\n",
      "('Test data array looks like this:', (137, 700))\n"
     ]
    }
   ],
   "source": [
    "job = train_and_test(movie_text, boxoffice_range, movie_title)\n",
    "job.split_train_test(300)\n",
    "job.feature_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training the data\n"
     ]
    }
   ],
   "source": [
    "job.train_randomForest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     box-office\n",
      "0             2\n",
      "1             3\n",
      "2             2\n",
      "3             2\n",
      "4             3\n",
      "5             3\n",
      "6             2\n",
      "7             3\n",
      "8             3\n",
      "9             2\n",
      "10            3\n",
      "11            2\n",
      "12            2\n",
      "13            3\n",
      "14            3\n",
      "15            2\n",
      "16            3\n",
      "17            3\n",
      "18            2\n",
      "19            3\n",
      "20            2\n",
      "21            2\n",
      "22            2\n",
      "23            3\n",
      "24            3\n",
      "25            3\n",
      "26            2\n",
      "27            3\n",
      "28            2\n",
      "29            2\n",
      "..          ...\n",
      "107           2\n",
      "108           2\n",
      "109           2\n",
      "110           2\n",
      "111           3\n",
      "112           3\n",
      "113           2\n",
      "114           3\n",
      "115           2\n",
      "116           3\n",
      "117           2\n",
      "118           2\n",
      "119           3\n",
      "120           3\n",
      "121           3\n",
      "122           2\n",
      "123           3\n",
      "124           2\n",
      "125           3\n",
      "126           3\n",
      "127           2\n",
      "128           3\n",
      "129           3\n",
      "130           3\n",
      "131           3\n",
      "132           2\n",
      "133           3\n",
      "134           3\n",
      "135           3\n",
      "136           2\n",
      "\n",
      "[137 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "job.make_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.583941605839\n"
     ]
    }
   ],
   "source": [
    "job.score()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
